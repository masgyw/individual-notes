# 分布式系统

## 什么是分布式系统
1. 为什么要进行系统拆分？  
单体应用，几十万行代码的上线，代码互相依赖，互相调用，很容易发生代码冲突；  
部署发布时，所有代码一起发布，各个模块可能互相影响，所有人都得负责；  
一个人修改代码，就得测试其他代码；  
一个人部署，所有人都得一起部署；  
一个人修改技术版本，所有项目都受影响。  
拆分了以后，服务相互独立，每个人维护自己的模块。
2. 如何进行系统拆分？  
1）系统拆分分布式服务，拆分成多个服务，拆成微服务架构，拆很多轮的。  
2）上来架构师拆好了第一轮，团队继续扩大，拆好的服务，该开始是一个人维护1w行代码，后来业务复杂了，服务变为10w行代码，5个人；第二轮，1个服务-》5个服务，每个服务2万行代码，每个人负责一个服务；  
如果多人维护一个服务，<=3 人维护，多了容易代码冲突；1个人负责1～3个服务，如果某个服务代码量大了，就要进行拆分，增加人员维护。  
所以，拆分系统是多轮的，不是一次就能解决的。
3. 拆分后不用dubbo可以吗？  
当然可以，基于http 请求；但接口超时、负载均衡、网络通信也得自己做，但使用dubbo可以快速解决上述问题，dubbo是rpc框架，本地进行接口调用。
4. dubbo和thrift有什么区别呢？  

## CAP理论及BASE理论
CAP：一致性、可用性、分区容忍性，CP或AP，三者不可兼得  
BASE：基本可用、软状态、最终一致性，基于CAP理论，AP基础上，软件允许中间状态不一致，但保证最终一致性

## 分布式系统如何负载均衡？如何确定访问的资源在哪个服务器上？
1）轮询  
2）随机：权重  
3）最小响应时间  
4）最小并发数  
5）hash/一致性hash  

资源定位：  
1）维护关系映射：可以根据服务器的ip和资源的唯一标示形成映射，请求时就可以确定服务器，或者存入数据库中，资源的服务器位置；  
2）一致性hash：资源存储和资源访问，使用一致的hash策略  
## 设计一个分布式负载均衡缓冲系统，如何快速定位到是那个服务器

## 如何保证缓冲区和数据库之间的强一致性
1. 唯一标识的请求，到相同的服务
2. 服务生成内存队列，封装读和写请求，操作串行
3. 写操作：先删缓存，后写库
4. 读操作：先读缓存，后读库
5. 操作标志，相同读请求，只入队一次，循环等待200ms；200ms 无响应，查询数据库，入队更新缓存；

---
## 分布式服务框架
### Dubbo 和 SpringCloud对比
底层：底层架构原理是类似的
|项目|Dubbo|SpringCloud|
|---|---|---|
|注册中心|Zookeeper，CP|Eurake，AP|
|底层协议|RPC协议，性能更好，并发|http协议|
|模型|接口即服务|应用即服务|
说明：
1. 中小型公司，其实没有那么高的并发，使用http也满足需求
2. Dubbo只是一个服务框架，SpringCloud而是一种为服务解决方案，提供了一整套组件，功能齐全，例如网关/配置中心/链路追踪等
3. SpringCloud alibaba 整合阿里的功能
### 服务注册中心怎么选型的，为什么选这个？
1. 服务注册发现原理  
Eureka：集群模式，每个机器地位对等，注册：数据同步；发现：有延迟，数据同步后才会发现  
ZK：集群模式，注册，leader写，同步到follower上，follower只读；发现：通过监听机制通知所有机器，有新的机器上线和下线
2. 一致性保障  
ZK：CP，leader接收到数据，会原子广播给其他节点，如果leader挂了，会选举出新的leader节点，此时服务不可用，保证了C，leader选取好了，继续提供服务  
Eureka:AP，peer模式，数据若没同步，其他服务器的注册表是旧的，但是服务依然可用，一段时间后，实现最终一致性  
3. 服务发现的时效性
ZK：Watch 机制，s级  
Eureka：默认配置，服务发现，时效性很差；服务故障：每60s判断心跳，超过90s服务无心跳，下线
4. 容量
容量都不高  
ZK：千级别，服务更新，大量请求通知，可能瞬间打满内存/带宽  
Eureka：大量注册，就有大量心跳，服务注册，每台机器都是对等的，所以每台机器的压力都是一样的，不能支撑大规模的连接
### 如果要部署上万台服务实例，注册中心是否能抗住？如何优化？
1. ZK在服务注册时，瞬时通知所有其他服务，造成网络带宽压力过大
2. Eureka 每台机器是对等的，包含所有的注册表信息，拉取全量注册表；全量的心跳，导致系统资源消耗
3. 优化（类似Redis cluster）：
    - 集群部署，分片存储
    - 主从结构，高可用
    - 局部拉取，只拉取自己需要的，本地存储
    - 强一致性，写到Slaver才算成功
### 服务发现过慢的问题，怎么优化和解决
Eureka 集群部署  
1. 修改ReadWrite缓存同步到ReadOnly缓存的周期：
eureka.server.responseCacheUpdateIntervalMs
2. 从eureka服务端获取注册信息的间隔时间：eureka.client.registryFetchIntervalSeconds = 30000
3. eureka客户端向服务端发送心跳的时间间隔：
eureka.client.leaseRenewalIntervalInSeconds
4. 将客户端剔除的服务在服务注册列表中剔除时间间隔：eureka.server.evictionIntervalTimerInMs
5. 续约到期时间，心跳过期时间：eureka.instance.leaseExpirationDurationInSeconds

### 网关选型
- 核心功能  
1）动态路由：服务增减，网关自动感知，不需要重新部署  
2）灰度发布：获取存储的灰度发布配置，根据请求url判断是哪个服务，是否启用灰度发布，若启用灰度发布，新服务参数设置为new，根据随机数区域配置，将99%的请求发送到旧服务上，1%请求发送到新服务上，没问题后，再将新服务全部部署  
3）权限验证  
4）资源隔离/限流  
5）性能监控
- 选型参考  
Kong/zuul/nginx+lua/自研网关  
1）Kong:Nginx 里面一个基于lua写的模块，实现了网关功能  
2）Zuul:SpringCloud，简单的功能，优势：基于java 做二次开发，有源码可以直接修改  
3）nginx+lua：基于nginx 开源  
中小公司：直接基于nginx反向代理+负载均衡
- 如果网关需要抗高并发？如何优化？  
1）zuul 集群部署  
2）nginx反向代理  
3）LVS 软负载均衡  
4）优化：基于DB做动态路由，灰度发布

### 怎么配置超时和重试机制参数的，为什么这么配置？
1. 第1次请求时，通常会发生超时
    - 原因：ribbon 懒加载拉取注册表
    - 优化：ribbon.eager-load.enable=true，关闭懒加载；zuul 中也是；
    - 跟机器性能相关、网络带宽等有联系
2. 超时、重试机制参数设置
    - ribbon.connectTimeout：1000，连接超时时间
    - ribbon.readTimeout：1000，请求超时
    - ribbon.maxAutoRetries:1,重试当前服务1次
    - ribbon.maxAutoRetriesNextKey:1，出哦功能是其他服务1次
3. zuul重试
    - zuul.retryable: true

---
## 分布式ID
好的id满足条件：  
1）纯数字，存储空间小  
2）有序  
3）趋势递增，便于作为数据库主键  

|方案|说明|优势|劣势|
|---|---|---|---|
|UUID|类库|1.本地生成，生成简单<br>|1.纯字符串，存储性能差，查询效率差<br>2.过长，占用存储空间大<br>3.ID可读性差<br>4.有信息泄露问题，泄露mac地址|
|单点数据库自增|单机|1.实现简单，数据库即可<br>2.数字，单调递增<br>3.客户与业务整合，可读性<br>|1.单点故障<br>2.数据库压力大，不能高并发|
|高可用数据库|主从|1.固定步长，步长>节点数，高可用<br>2.平衡负载|1.扩容难<br>2.数据库支持的并发数一定<br>3.主从同步延迟|
|leaf-segment|每次获取ID区间|1.扩张灵活，支持高并发<br>2.ID趋势递增<br>3.可用性高，数据库不可用，有本地缓存|1.多个节点并发请求获取ID|
|双buff leaf-segment|增加一个缓冲区间，<br>当前到达10%，<br>更新下一个缓冲区间|减少了数据并发的冲突，减少<br>网络依赖，效率更高|segment可能因为业务频繁更新|
|基于redis、zk等中间件生成|动态生成|1.递增<br>2.高并发支持|依赖第三方中间件的可用性|
|雪花算法|64bit整型数字<br>0+41时间戳+10机器码+12序列号|1.每个毫秒存在万个ID<br>2.递增<br>3.根据业务灵活设计bit的标识|1.依赖机器时钟，若时钟回拨，会导致ID重复；一旦回拨，则不可再生成ID|

## 分布式锁
1. 数据库实现
- 原理  
1）加锁：insert into ，插入成功则加锁；否则加锁失败
2）解锁：删除指定主键的记录
- 缺点：
    - 非阻塞：轮询插入，线程不会被其他任务使用
    - 非重入：不可重入
    - 单点：单点故障
    - 锁超时：解锁时，数据库不可用，导致锁一直存在，服务不可用
2. redis 实现  
- 原理  
set key {randomNUm} NX PX 30000  
原子设置key，不存在则创建，并设置随机值和过期时间  
1）加锁：set 成功就是加锁成功了  
2）释放锁：判断线程是否为占有者，若是，删除key；一般可以用lua 脚本删除，判断value为占有者才删除
- 缺点：  
    - 锁超时：任务超时，锁自动释放，导致并发问题，加锁和释放锁非同一线程  
    - 并发问题：集群模式，主节点加锁成功后，宕机，从节点未及时同步，导致其他应用加锁成功  
- 集群RedLock 算法  
条件：redis cluster 集群，5个实例  
1）获取当前时间戳，单位毫秒  
2）轮流在每个master节点创建锁，过期时间较短，一般几十毫秒  
3）尝试在大多数节点上创建锁  
4）客户端计算建立好锁的时间，如果建立的锁的时间小于超时时间，则加锁成功；否则失败，依次删除这个锁  
5）只要别人创建了分布式锁，其他轮询尝试获取锁
- Java实现  
redisson工具类，实现锁重入、锁续期、redlock，自实现代码复杂
2. Zookeeper 实现  
1）加锁：创建临时节点成功（也可以使用临时顺序节点）  
2）释放锁：删除临时节点，通知所有监听者  
3）阻塞：若获取锁失败，阻塞并监听临时节点，等待zk通知临时节点删除，重新获取锁  
4）解决锁超时问题：客户端宕机，session断开，临时节点删除，其他客户端可获取锁  
5）重入性：节点存储当前线程，获取最小序号节点内容，若和当前节点一致，重入  
缺点：
    - 锁不公平，谁能加锁是不确定  
3. 优劣势对比：  
数据库方案，缺点多，性能差，不考虑  
1、redis redLock 算法的复杂、健壮性不好；  
2、redis 锁需要循环遍历，需要消耗网络性能；zk 基于事件通知，性能更好；  

## 分布式会话
- 无状态服务：服务无状态，不需要分布式会话
- 前端存储cookie
- IP绑定策略：Nginx 可以设定ip绑定，同一个ip指定机器访问
- 分布式存储机制  
1）Tomcat + Redis  
在Tomcat 里配置插件，让容器存储Session 到redis 中，读写都从 redis 中取  
缺点：和web容器高度耦合，不易切换web容器  
2）SpringSession + Redis  
通过SrpingSession 将 session 数据存储到redis 中，完成  
3）好处：分布式session、水平扩容

## 分布式事务  
1. 本地事务  
事务ACID：原子性、一致性、隔离性、持久性
2. 分布式事务  
原因：多个系统完成某个交易，需要所有系统都完成，只要有一个系统失败，则交易失败，事务回滚。
3. 概念
- XA：XA规范：分布式事务规范，定义了分布式事务模型
- JTA：Java对XA规范的实现
- 四个角色：事务管理器(协调者TM)、资源管理器(参与者RM)，应用程序AP，通信资源管理器CRM
4. 解决方案  

|方案|说明|缺点|适用场景|
|---|---|---|---|
|两阶段提交（XA）|1.RM执行本地事务，但未提交，<br>2.所有RM都提交成功，发送commit，否则发送fail回滚|1.严重依赖数据库，效率很低；<br>2.单点故障，TM宕机，RM都会阻塞；数据不一致，若阶段二，发送commit消息，参与者没有收到<br>3.规范：每个服务职能操作自己对应的数据库，如果要操作别的数据库，不能直连别的数据库，违反微服务的服务架构；如果要操作别的库，必须调用其他服务接口，不允许交叉访问；<br>4.低性能；事务时间长，资源锁时间长；|不允许使用！<br>一个系统协调多个库，跨库操作|
|三阶段提交|针对两阶段优化，解决单点故障<br>1.canCommit<br>2.preCommit<br>3.doCommit|1.数据不一致<br>2.性能差|与两阶段一样|
|TCC|try-confirm-cancel,一种补偿方案|与业务重耦合的方案，try-confirm-cancel都与业务强相关，复杂度高，代码量大，难维护|1.confirm和cancel接口幂等<br>2.与资金相关的、支付、交易相关，严格保证分布式事务，要么成功、要么失败|
|本地消息表|依赖消息表+MQ|大量依赖数据库消息表，高并发场景性能不好，无法扩展|很少使用||
|可靠消息最终一致性方案|||用的比较多|
|最大努力通知||存在分布式事务失败的情况|用的比较少，多次重试，可以在一定程度上允许少数分布式事务失败，一般是对分布式事务要求不严格的情况下，比如日志记录|

5. 实现详情：  
I） XA
2PC、3PC  
II）TCC  
TCC的全程是：Try、Confirm、Cancel  
1）Try阶段：这个阶段说的是对各个服务的资源做检测以及对资源进行锁定或者预留  
2）Confirm阶段：这个阶段说的是在各个服务中执行实际的操作  
3）Cancel阶段：如果任何一个服务的业务方法执行出错，那么这里就需要进行补偿，就是执行已经执行成功的业务逻辑的回滚操作  
III）本地消息表  
1）A系统在自己本地一个事务里操作同时，插入一条数据到消息表  
2）接着A系统将这个消息发送到MQ中去  
3）B系统接收到消息之后，在一个事务里，往自己本地消息表里插入一条数据，同时执行其他的业务操作，如果这个消息已经被处理过了，那么此时这个事务会回滚，这样保证不会重复处理消息  
4）B系统执行成功之后，就会更新自己本地消息表的状态以及A系统消息表的状态  
5）如果B系统处理失败了，那么就不会更新消息表状态，那么此时A系统会定时扫描自己的消息表，如果有没处理的消息，会再次发送到MQ中去，让B再次处理  
6）这个方案保证了最终一致性，哪怕B事务失败了，但是A会不断重发消息，直到B那边成功为止  
IV）可靠消息最终一致性方案  
基于RocketMQ 的消息事务  
1）A系统先发送一个prepared消息到mq，如果这个prepared消息发送失败那么就直接取消操作别执行了  
2）如果这个消息发送成功过了，那么接着执行本地事务，如果成功就告诉mq发送确认消息，如果失败就告诉mq回滚消息  
3）如果发送了确认消息，那么此时B系统会接收到确认消息，然后执行本地的事务  
4）mq会自动定时轮询所有prepared消息回调你的接口，问你，这个消息是不是本地事务处理失败了，所有没发送确认消息？那是继续重试还是回滚？一般来说这里你就可以查下数据库看之前本地事务是否执行，如果回滚了，那么这里也回滚吧。这个就是避免可能本地事务执行成功了，别确认消息发送失败了。  
5）这个方案里，要是系统B的事务失败了咋办？重试咯，自动不断重试直到成功，如果实在是不行，要么就是针对重要的资金类业务进行回滚，比如B系统本地回滚后，想办法通知系统A也回滚；或者是发送报警由人工来手工回滚和补偿  
V）最大努力通知方案  
1）系统A本地事务执行完之后，发送个消息到MQ  
2）这里会有个专门消费MQ的最大努力通知服务，这个服务会消费MQ然后写入数据库中记录下来，或者是放入个内存队列也可以，接着调用系统B的接口  
3）要是系统B执行成功就ok了；要是系统B执行失败了，那么最大努力通知服务就定时尝试重新调用系统B，反复N次，最后还是不行就放弃，放入DL  

6. 你们公司是如何使用分布式事务的？怎么选型的？  
    - 最常用的是TCC和可靠消息的最终一致性  
    - 严格资金要求绝对不能错的场景，你可以说你是用的TCC方案；如果是一般的分布式事务场景，订单插入之后要调用库存服务更新库存，库存数据没有资金那么的敏感，可以用可靠消息最终一致性方案（rocketmq 3.2.6之前的版本）  
    - 类似TCC开源框架，ByteTCC，himly，阿里开源的seata，建议使用阿里seata，经历阿里大量考验的框架，支持dubbo/springCloud，ByteTCC纯正的TCC框架
    - 可靠消息最终一致性：基于rabitMQ/rocketMQ自己封装一个可靠消息服务，受到消息后，尝试投递到MQ上，投递失败，重复投递；消费成功后，必须回调一个接口，通知处理成功，一段时间若还是没有处理成功，再次投递消息到MQ
7. 分布式事务是否需要大量使用？  
结论：不要在系统中大量使用，需要平衡选择，1%的业务使用分布式事务，99%的业务是不要使用的。trade off，权衡，要用分布式事务的时候，一定是有成本；好处，如果做好了，TCC、可靠消息最终一致性方案，一定可以100%保证你那快数据不会出错。  
原因：  
1）复杂度高  
2）代码量大  
3）性能差  
4）系统吞吐量/性能大幅下降  
5）系统更加复杂且容易出bug
解决方案：  
99%的业务分布式接口调用，不要用分布式事务，直接就是监控（发邮件）、记录日志、事后快速定位、排查和出解决方案、修复数据。  
正常情况下，生产bug 是很少的，每次发生后，修复bug/数据，成本远远比分布式事务来的低，而且分布式事务的bug也容易发生。  

## 分布式通信
RPC：远程过程调用，在本地调用远程函数，可以实现跨语言实现  
RMI：远程方法调用，Java版RPC，面向对象的思维方式，远程调用跨JVM的对象方法  

## 如何保证接口幂等
分插入类和更新类，区别处理，总共两种方案：
- 插入类：数据库唯一索引
- 更新类：redis 缓存机制
    - 实现方案：基于redis的防重框架
        1. 基于springmvc的拦截器，根据约定获取请求的拦截参数，将请求参数拼接在一起，作为key
        2. key到redis 中判断是否存在，若不存在，则存入；若存在，不请求；
    - 缺点：是否要对所有请求都要去重？接口的确超时失败了，去重导致重试直接返回了？请求之后，接口超时成功了，但第二个请求，还是成功了
    - 建议
        1. 核心接口才做接口防重
        2. 每个服务根据自己的业务来定制化接口防重机制，而不是使用通用的，例如扣减库存服务，可以在扣减成功后，写入redis，重试请求成功后，写入失败，业务回滚；
1. 唯一ID：每次操作，都根据操作和内容生成唯一id，在执行前判断id是否存在，若不存在，则进行后续操作，并保存到存储容器中（不太实用）
    - 缺点：每个服务都需要实现该功能逻辑，工作量重复；若服务写入id后，挂了，需要引入超时机制；
2. 插入或更新：在数据中插入并有唯一索引的情况下使用
    - mysql：insert into ...values() on DUPLICATE KEY UPDATE
3. 去重表：业务唯一标识保存在数据表中，唯一索引，重复执行，执行失败
4. 版本控制：适合更新场景，增加版本号，若版本号一致，则更新
    - 缺点：范围更新，不可使用，适合唯一索引更新，行锁
5. 状态控制：如订单状态，未支付、支付中、支付失败，只能由未支付变成支付中  

银行对外提供的付款接口保证幂等：
source来源+seq序列号，做唯一索引，防止重复付款

## 如何设计一个高并发的系统
1）系统拆分，分布式系统  
2）缓存  
3）MQ  
4）分库分表  
5）读写分离  
6）ES 集群

## 每天请求量，成功，失败次数，QPS
核心接口，开发简单的metric统计机制，原子类，统计接口当天访问量。  
压测最大请求数：使用压测工具，200以内的请求是没有问题的。  
接口性能：  
TP99，99%的请求都在100ms以内  
TP95，95%的请求都在50ms以内  
平均响应延时  

## 序列化框架选型
||JDK|Hessian|Protobuf|
|---|---|---|---|
|优点|使用方便，包含信息多<br>安全性高|产生码流较小<br>跨语言|产生码流小<br>跨语言<br>速度快<br>灵活性高|
|缺点|产生码流过大<br>网络传输带宽大<br>消耗性能<br>不支持跨语言|比JDK效率好<br>性能不高|需要环境安装和搭建|